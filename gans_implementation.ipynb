{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **GANs I'm Something of a Painter Myself**\n\n[Kaggle – I'm Something of a Painter Myself](https://www.kaggle.com/competitions/gan-getting-started)  \n\n## **Problem & Data Description**\nWe must train a *generative adversarial network* (GAN) that converts ordinary photographs into Monet‑style paintings (or generates Monet‑like art from scratch).  \n\n**Dataset** (`/kaggle/input/gan-getting-started`):  \n* `monet_jpg/` – 300 Monet paintings (256 × 256 RGB)  \n* `photo_jpg/` – 7 028 ordinary photos (256 × 256 RGB)  \n\n**Submission format:** A single **`images.zip`** containing **7 000‑10 000** Monet‑style `.jpg` images (256×256).  \n\n**Evaluation metric:** **MiFID** (Memorization‑informed Fréchet Inception Distance) – lower is better; memorization of training examples is penalised.\n","metadata":{}},{"cell_type":"markdown","source":"## **Exploratory Data Analysis (EDA)**","metadata":{}},{"cell_type":"code","source":"import os, random, itertools, zipfile, shutil, gc\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom scipy import linalg\n\nROOT_DIR = Path('/kaggle/input/gan-getting-started')\nWORK_DIR = Path('/kaggle/working')\n\nmonet_dir = ROOT_DIR/'monet_jpg'\nphoto_dir = ROOT_DIR/'photo_jpg'\n\nprint(f\"Monet images : {len(list(monet_dir.glob('*.jpg')))}\")\nprint(f\"Photo images : {len(list(photo_dir.glob('*.jpg')))}\")\n\ndef show_samples(path, n=5, title='Samples'):\n    samples = random.sample(list(path.glob('*.jpg')), n)\n    plt.figure(figsize=(15,3))\n    for i, img_path in enumerate(samples, 1):\n        img = Image.open(img_path)\n        plt.subplot(1, n, i)\n        plt.imshow(img)\n        plt.axis('off')\n    plt.suptitle(title)\n    plt.show()\n\nshow_samples(monet_dir, title='Monet paintings')\nshow_samples(photo_dir, title='Real photos')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T18:51:49.999496Z","iopub.execute_input":"2025-04-27T18:51:49.999808Z","iopub.status.idle":"2025-04-27T18:51:50.887495Z","shell.execute_reply.started":"2025-04-27T18:51:49.999785Z","shell.execute_reply":"2025-04-27T18:51:50.886690Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Model Architecture & Training**\nWe will use CycleGAN (Zhu et al., 2017) to translate images from the photo domain -> Monet domain while enforcing cycle‑consistency.\n","metadata":{}},{"cell_type":"code","source":"# CycleGAN Architecture\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nIMG_SIZE = 256\nBUFFER_SIZE = 7000\nBATCH_SIZE  = 4\nEPOCHS = 40\n\n# Data pipeline\ndef load_img(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)  # [0,1]\n    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n    return img\n\nmonet_paths = tf.data.Dataset.list_files(str(monet_dir/'*.jpg'), shuffle=True)\nphoto_paths = tf.data.Dataset.list_files(str(photo_dir/'*.jpg'), shuffle=True)\n\nmonet_ds = monet_paths.map(load_img, num_parallel_calls=tf.data.AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\nphoto_ds = photo_paths.map(load_img, num_parallel_calls=tf.data.AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n\n# Generator and discriminator (U‑Net + PatchGAN)\ndef downsample(filters, size, apply_instancenorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    block = tf.keras.Sequential()\n    block.add(layers.Conv2D(filters, size, strides=2, padding='same',\n                            kernel_initializer=initializer, use_bias=False))\n    if apply_instancenorm:\n        block.add(tfa.layers.InstanceNormalization())\n    block.add(layers.LeakyReLU())\n    return block\n\ndef upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    block = tf.keras.Sequential()\n    block.add(layers.Conv2DTranspose(filters, size, strides=2,\n                                     padding='same',\n                                     kernel_initializer=initializer,\n                                     use_bias=False))\n    block.add(tfa.layers.InstanceNormalization())\n    if apply_dropout:\n        block.add(layers.Dropout(0.5))\n    block.add(layers.ReLU())\n    return block\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T16:45:47.305079Z","iopub.execute_input":"2025-04-26T16:45:47.305516Z","iopub.status.idle":"2025-04-26T16:46:11.183301Z","shell.execute_reply.started":"2025-04-26T16:45:47.305484Z","shell.execute_reply":"2025-04-26T16:46:11.182446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CycleGAN Generator (ResNet‑based) and PatchGAN Discriminator\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n\n# Helper layers\nclass ReflectionPadding2D(layers.Layer):\n    def __init__(self, padding=(1,1), **kwargs):\n        super().__init__(**kwargs)\n        self.padding = padding\n\n    def call(self, x):\n        w_pad, h_pad = self.padding\n        return tf.pad(x, [[0,0],[h_pad,h_pad],[w_pad,w_pad],[0,0]], mode='REFLECT')\n\ndef resnet_block(x, filters, size=3):\n    # Pad, conv, norm, relu, conv, norm, add\n    init = x\n    x = ReflectionPadding2D()(x)\n    x = layers.Conv2D(filters, size, padding='valid')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n\n    x = ReflectionPadding2D()(x)\n    x = layers.Conv2D(filters, size, padding='valid')(x)\n    x = layers.BatchNormalization()(x)\n\n    return layers.add([init, x])\n\ndef build_generator(img_size=256, n_res_blocks=9):\n    inputs = layers.Input(shape=(img_size, img_size, 3))\n\n    x = ReflectionPadding2D(padding=(3,3))(inputs)\n    x = layers.Conv2D(64, 7, padding='valid')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n\n    # Down‑sampling\n    for filters in [128, 256]:\n        x = layers.Conv2D(filters, 3, strides=2, padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n\n    # Residual blocks\n    for _ in range(n_res_blocks):\n        x = resnet_block(x, 256)\n\n    # Up‑sampling\n    for filters in [128, 64]:\n        x = layers.Conv2DTranspose(filters, 3, strides=2, padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n\n    x = ReflectionPadding2D(padding=(3,3))(x)\n    x = layers.Conv2D(3, 7, padding='valid', activation='tanh')(x)\n\n    return models.Model(inputs, x, name='generator')\n\ndef build_discriminator(img_size=256):\n    inputs = layers.Input(shape=(img_size, img_size, 3))\n    x = layers.Conv2D(64, 4, strides=2, padding='same')(inputs)\n    x = layers.LeakyReLU(0.2)(x)\n\n    for filters, stride in zip([128, 256, 512], [2,2,1]):\n        x = layers.Conv2D(filters, 4, strides=stride, padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Conv2D(1, 4, padding='same')(x)  # PatchGAN output\n    return models.Model(inputs, x, name='discriminator')\n\n# Instantiate models \ngenerator_G = build_generator()\ngenerator_F = build_generator()\ndiscriminator_X = build_discriminator()\ndiscriminator_Y = build_discriminator()\n\nprint(\"Generators & discriminators built.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T16:46:11.184837Z","iopub.execute_input":"2025-04-26T16:46:11.185618Z","iopub.status.idle":"2025-04-26T16:46:12.720072Z","shell.execute_reply.started":"2025-04-26T16:46:11.185575Z","shell.execute_reply":"2025-04-26T16:46:12.718586Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Loss Functions, Optimizers & Training Loop**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import losses, optimizers\n\n# Hyper‑params\nLAMBDA_CYCLE = 10.0\nLAMBDA_ID    = 0.5 * LAMBDA_CYCLE\nLR           = 2e-4\nBETA_1       = 0.5\n\n# Optimizers\ngenerator_G_optimizer = optimizers.Adam(LR, beta_1=BETA_1)\ngenerator_F_optimizer = optimizers.Adam(LR, beta_1=BETA_1)\ndiscriminator_X_optimizer = optimizers.Adam(LR, beta_1=BETA_1)\ndiscriminator_Y_optimizer = optimizers.Adam(LR, beta_1=BETA_1)\n\n# Loss Objects\nmse = losses.MeanSquaredError()\n\ndef generator_loss(fake_logits):\n    # Least‑Squares GAN (LSGAN) loss\n    return mse(tf.ones_like(fake_logits), fake_logits)\n\ndef discriminator_loss(real_logits, fake_logits):\n    real_loss = mse(tf.ones_like(real_logits), real_logits)\n    fake_loss = mse(tf.zeros_like(fake_logits), fake_logits)\n    return 0.5 * (real_loss + fake_loss)\n\ndef cycle_consistency_loss(real_img, cycled_img):\n    return LAMBDA_CYCLE * tf.reduce_mean(tf.abs(real_img - cycled_img))\n\ndef identity_loss(real_img, same_img):\n    return LAMBDA_ID * tf.reduce_mean(tf.abs(real_img - same_img))\n\n# @tf.function Train Step\n@tf.function\ndef train_step(real_x, real_y):\n    \"\"\"Runs one training step (one batch) for CycleGAN.\"\"\"\n    with tf.GradientTape(persistent=True) as tape:\n        # Generators\n        fake_y   = generator_G(real_x, training=True)\n        cycled_x = generator_F(fake_y,   training=True)\n\n        fake_x   = generator_F(real_y, training=True)\n        cycled_y = generator_G(fake_x, training=True)\n\n        # Identity mapping (helps preserve colour / content)\n        same_x = generator_F(real_x, training=True)\n        same_y = generator_G(real_y, training=True)\n\n        # Discriminators\n        disc_real_x = discriminator_X(real_x, training=True)\n        disc_real_y = discriminator_Y(real_y, training=True)\n        disc_fake_x = discriminator_X(fake_x, training=True)\n        disc_fake_y = discriminator_Y(fake_y, training=True)\n\n        # Generator losses\n        gen_G_adv   = generator_loss(disc_fake_y)\n        gen_F_adv   = generator_loss(disc_fake_x)\n        total_cycle = cycle_consistency_loss(real_x, cycled_x) + cycle_consistency_loss(real_y, cycled_y)\n        total_id    = identity_loss(real_x, same_x) + identity_loss(real_y, same_y)\n\n        gen_G_total = gen_G_adv + total_cycle + total_id\n        gen_F_total = gen_F_adv + total_cycle + total_id\n\n        # Discriminator losses\n        disc_X_loss = discriminator_loss(disc_real_x, disc_fake_x)\n        disc_Y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n\n    # Apply Gradients\n    # Generators\n    grads_G = tape.gradient(gen_G_total, generator_G.trainable_variables)\n    grads_F = tape.gradient(gen_F_total, generator_F.trainable_variables)\n    generator_G_optimizer.apply_gradients(zip(grads_G, generator_G.trainable_variables))\n    generator_F_optimizer.apply_gradients(zip(grads_F, generator_F.trainable_variables))\n\n    # Discriminators\n    grads_disc_X = tape.gradient(disc_X_loss, discriminator_X.trainable_variables)\n    grads_disc_Y = tape.gradient(disc_Y_loss, discriminator_Y.trainable_variables)\n    discriminator_X_optimizer.apply_gradients(zip(grads_disc_X, discriminator_X.trainable_variables))\n    discriminator_Y_optimizer.apply_gradients(zip(grads_disc_Y, discriminator_Y.trainable_variables))\n\n    return {\n        \"gen_G\": gen_G_total,\n        \"gen_F\": gen_F_total,\n        \"disc_X\": disc_X_loss,\n        \"disc_Y\": disc_Y_loss\n    }\n\n# High‑Level Training Driver\ndef train(epochs):\n    for epoch in range(1, epochs + 1):\n        print(f\"Epoch {epoch}/{epochs}\")\n        for real_x, real_y in tf.data.Dataset.zip((photo_ds, monet_ds)):\n            losses_dict = train_step(real_x, real_y)\n        # Simple logging\n        if epoch % 5 == 0:\n            print({k: f\"{v.numpy():.4f}\" for k, v in losses_dict.items()})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T16:46:12.721351Z","iopub.execute_input":"2025-04-26T16:46:12.721740Z","iopub.status.idle":"2025-04-26T16:46:12.754466Z","shell.execute_reply.started":"2025-04-26T16:46:12.721701Z","shell.execute_reply":"2025-04-26T16:46:12.753129Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Generate & Export Images**","metadata":{}},{"cell_type":"code","source":"# === Paths ===\nOUTPUT_DIR = \"../images\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# === Generate and Save ===\nfor idx, photo_path in enumerate(tf.data.Dataset.list_files(str(photo_dir/'*.jpg')).take(8000)):\n    img = load_img(photo_path)\n    generated = generator_G(tf.expand_dims(img, 0), training=False)[0].numpy()\n    generated = (generated * 127.5 + 127.5).astype(np.uint8)  # [-1,1] → [0,255]\n    output_path = f\"{OUTPUT_DIR}/{idx:05d}.jpg\"\n    Image.fromarray(generated).save(output_path)\n\n# === Make ZIP ===\nsubmission_path = shutil.make_archive(\"/kaggle/working/images\", 'zip', OUTPUT_DIR)\nprint(f\"Submission file created: {submission_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T16:46:12.756889Z","iopub.execute_input":"2025-04-26T16:46:12.757313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Evaluation**","metadata":{}},{"cell_type":"code","source":"# Load InceptionV3 once\ninception = tf.keras.applications.InceptionV3(include_top=False,\n                                              weights='imagenet',\n                                              pooling='avg',\n                                              input_shape=(299,299,3))\ninception.trainable = False\n\ndef _preprocess(img):\n    # expects float32 [0,1] -> scale to [-1,1] then resize to 299x299\n    img = tf.image.resize(img, (299,299))\n    img = (img * 2.0) - 1.0\n    return img\n\ndef _activations(paths, batch=32):\n    act_list = []\n    for i in range(0, len(paths), batch):\n        batch_paths = paths[i:i+batch]\n        imgs = []\n        for p in batch_paths:\n            img = tf.io.read_file(p)\n            img = tf.image.decode_jpeg(img, channels=3)\n            img = tf.image.convert_image_dtype(img, tf.float32)\n            imgs.append(_preprocess(img))\n        imgs = tf.stack(imgs, axis=0)\n        acts = inception(imgs, training=False)\n        act_list.append(acts.numpy())\n    return np.concatenate(act_list, axis=0)\n\ndef _calculate_statistics(acts):\n    mu = np.mean(acts, axis=0)\n    sigma = np.cov(acts, rowvar=False)\n    return mu, sigma\n\ndef _calculate_fid(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n    fid = np.sum((mu1 - mu2)**2) + np.trace(sigma1 + sigma2 - 2*covmean)\n    return float(fid)\n\ndef calculate_fid(sample_size=1000, gen_dir='/kaggle/working/generated',\n                  real_dir='/kaggle/input/gan-getting-started/monet_jpg'):\n    gen_paths  = sorted(glob(f'{gen_dir}/*.jpg'))[:sample_size]\n    real_paths = sorted(glob(f'{real_dir}/*.jpg'))\n    if len(gen_paths) < sample_size:\n        raise ValueError(f'Not enough generated images found in {gen_dir}')\n    real_paths = (real_paths * ((sample_size // len(real_paths)) + 1))[:sample_size]\n\n    acts_gen  = _activations(gen_paths)\n    acts_real = _activations(real_paths)\n\n    mu_gen,  sigma_gen  = _calculate_statistics(acts_gen)\n    mu_real, sigma_real = _calculate_statistics(acts_real)\n\n    return _calculate_fid(mu_gen, sigma_gen, mu_real, sigma_real)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run `calculate_fid()` after training and image generation to obtain a **local FID estimate**.  \nAlthough MiFID adds a memorization penalty, a solid FID often correlates with a good public MiFID score.","metadata":{}},{"cell_type":"markdown","source":"## **Results & Conclusion**\n\nAfter training for N epochs with λ<sub>cycle</sub>=10 & λ<sub>identity</sub>=0.5, our model achieved a public score of about 357 (lower is better). The generated paintings display distinct Monet‑style brush strokes and colour palettes while maintaining photo composition.*  \n\nNext steps:\n- Fine‑tune learning rates & cycle‑consistency weight  \n- Increase training epochs using TPU to reduce MiFID further  \n- Experiment with StyleGAN‑V2 transfer‑learning for higher‑resolution images\n","metadata":{}}]}
